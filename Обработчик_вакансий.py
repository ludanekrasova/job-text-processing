# streamlit run –û–±—Ä–∞–±–æ—Ç—á–∏–∫_–≤–∞–∫–∞–Ω—Å–∏–π.py

import re

import joblib
import numpy as np
import pandas as pd
import streamlit as st
from catboost import CatBoostClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from tqdm import tqdm
#import xlsxwriter
import openpyxl


st.set_page_config(
    page_title="–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å–∏—Å—Ç–µ–º—ã —Ä–µ—à–µ–Ω–∏—è –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞–∫–∞–Ω—Å–∏–π",
    page_icon="üìã", layout="wide",
    initial_sidebar_state="expanded",
    menu_items={'Get Help': None,'Report a bug': None,'About': None})

hide_streamlit_style = """<style>#MainMenu {visibility: hidden;}footer {visibility: hidden;}</style>"""
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

############################################################################
# —Ä–∞–±–æ—Ç–∞ —Å —Ñ–∞–π–ª–∞–º–∏
############################################################################

@st.cache_data
def load_model():
    '''–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏'''
    # –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
    model.load_model('data/model.cbm')
    # –∑–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
    tfidf = joblib.load('data/tfidf.pkl')
    return model, tfidf


@st.cache_data
def load_data():
    '''–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞'''
    data = pd.read_excel('data/dataset.xlsx', engine='openpyxl').head(10)
    return data
def tfidf_featuring(tfidf, df):
    '''–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ –º–µ—à–æ–∫ —Å–ª–æ–≤'''
    X_tfidf = tfidf.transform(df['text'])
    feature_names = tfidf.get_feature_names_out()
    X_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=feature_names, index=df.index)
    return X_tfidf

def sentences_split(text):
    # —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    try:
        sentences = re.compile(r'\;|\.|\n|\‚Ä¢|‚Äî|–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏|—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è|—É—Å–ª–æ–≤–∏—è').split(text.lower())
        pattern = r'^[^–∞-—è–ê-–Ø—ë–Å]+|[^–∞-—è–ê-–Ø—ë–Å]+$'
        sentences = [re.sub(pattern, '', sen) for sen in sentences]
        return [sen for sen in sentences if len(sen) > 0]
    except:
        return []

def sentences_df(df, part=None):
    # –¥–∞—Ç–∞—Ñ—Ä–µ–π–º –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è, part —Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –∏–∑–≤–¥–ª–µ–∫–∞–µ–º
    test_ = []
    idxs = df.index.tolist()
    for idx in idxs[0:part]:
        text = df['responsibilities(–î–æ–ª–∂–Ω–æ—Å—Ç–Ω—ã–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏)'][idx]
        # print(sentences_split(text))
        test_.append(sentences_split(text))
    return test_

def sort_respons(sentences):
    requirements = []
    terms = []
    for idx in tqdm(range(0, len(sentences))):
        test_tfidf = tfidf_featuring(tfidf, pd.DataFrame({"text": sentences[idx]}))
        catc_proba = model.predict_proba(test_tfidf)
        temp_ = pd.DataFrame({"text": sentences[idx]})
        temp_['target'] = np.argmax(catc_proba, axis=1)
        temp_['proba'] = np.amax(catc_proba, axis=1)
        temp_['target'] = temp_['target'].replace({0: "–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏", 1: '–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è',
                                                   2: '–£—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã'}, regex=True)
        # –ø–æ–∫–∞ –±–µ–∑ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–µ–π
        # responsibilities = temp_[(temp_['target']=='–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏')&(temp_['proba']>=0.75)]['text'].tolist()
        requirements.append(temp_[(temp_['target'] == '–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è') & (temp_['proba'] >= 0.75)]['text'].tolist())
        terms.append(temp_[(temp_['target'] == '–£—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã') & (temp_['proba'] >= 0.75)]['text'].tolist())

    return requirements, terms

def print_list(text):
    # –≤—ã–≤–æ–¥ —Å–ø–∏—Å–∫–∞ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
    for i in text:
        st.text("- " + i.capitalize() + ";")


######################################################################################################
# –û—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥
######################################################################################################

model = CatBoostClassifier(loss_function='MultiClass', random_state=42)
tfidf = TfidfVectorizer()
# –∑–∞–≥—Ä—É–∑–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä–∞
model, tfidf = load_model()

#–∑–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
data = load_data()



# –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∏ —É—Å–ª–æ–≤–∏–π
sentences = sentences_df(data)
requirements, terms = sort_respons(sentences)

data['requirements(–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–æ–∏—Å–∫–∞—Ç–µ–ª—é)'] = requirements
data['terms(–£—Å–ª–æ–≤–∏—è)'] = terms

############################################################################
# –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
############################################################################

#result = result_default
counter_vac = data.shape[0] - 1
# —Å—Ç—Ä–µ–ª–æ—á–∫–∏ –¥–ª—è –≤–∞–∫–∞–Ω—Å–∏–π
button_container = st.container()
with button_container:
    if 'count' not in st.session_state:
        st.session_state.count = 0
    col1, col2, col3 = st.columns(3)
    with col1:
        decrement = st.button('‚Üê')
        if decrement and st.session_state.count > 0:
            st.session_state.count -= 1  # –£–º–µ–Ω—å—à–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ —Å—Ç—Ä–µ–ª–∫–∏ "‚Üê"
    with col2:
        idx = st.session_state.count
        st.write('–í–∞–∫–∞–Ω—Å–∏—è = ', st.session_state.count)
    with col3:
        increment = st.button('‚Üí')
        if increment and st.session_state.count < counter_vac:
            st.session_state.count += 1  # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–∏ –Ω–∞–∂–∞—Ç–∏–∏ —Å—Ç—Ä–µ–ª–∫–∏ "‚Üí"

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —è—á–µ–π–∫–∏ –ø–æ –∏–º–µ–Ω–∏ —Å—Ç–æ–ª–±—Ü–∞ –∏ –∏–Ω–¥–µ–∫—Å—É —Å—Ç—Ä–æ–∫–∏
    cell_value = data.at[st.session_state.count, 'responsibilities(–î–æ–ª–∂–Ω–æ—Å—Ç–Ω—ã–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏)']
    name_value = data.at[st.session_state.count, 'name(–Ω–∞–∑–≤–∞–Ω–∏–µ)']
    requirements_value = data.at[st.session_state.count,'requirements(–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Å–æ–∏—Å–∫–∞—Ç–µ–ª—é)']
    terms_value = data.at[st.session_state.count,'terms(–£—Å–ª–æ–≤–∏—è)']

    # –í—ã–≤–æ–¥ –∑–Ω–∞—á–µ–Ω–∏—è —è—á–µ–π–∫–∏

    st.markdown("<h4>–ù–∞–∑–≤–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏:</h4>", unsafe_allow_html=True)
    st.write(name_value)

    st.markdown("<h4>–ü–µ—Ä–≤–æ–Ω–∞—á–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç:</h4>", unsafe_allow_html=True)
    #st.subheader("–°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç:")
    st.write(cell_value)

    st.markdown("<h4>–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:</h4>", unsafe_allow_html=True)

    st.markdown("<h4>–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:</h4>", unsafe_allow_html=True)
    # —Å–ø–∏—Å–æ–∫ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    print_list(requirements_value)

    st.markdown("<h4>–£—Å–ª–æ–≤–∏—è:</h4>", unsafe_allow_html=True)
    # —Å–ø–∏—Å–æ–∫ —É—Å–ª–æ–≤–∏–π
    print_list(terms_value)

